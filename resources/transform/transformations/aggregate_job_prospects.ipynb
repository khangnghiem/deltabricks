{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a20c18-db36-4ea5-8b88-877bc39cae57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"SELECT * FROM dev.job_prospects.job_1900_silver\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35850f32-c4cc-4920-abc7-d5ec4b7bbdd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, col, datediff, max as spark_max\n",
    "\n",
    "# Get the max timestamp value\n",
    "max_timestamp = df.agg(spark_max(\"timestamp\").alias(\"max_ts\")).collect()[0][\"max_ts\"]\n",
    "\n",
    "# Filter last 30 days\n",
    "df_recent = df.filter(\n",
    "    (datediff(current_date(), col(\"posted_at\")) <= 30) &\n",
    "    (col(\"timestamp\") == max_timestamp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1855a3b4-7bb1-4dcb-8e78-28b34e227963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"timestamp\").orderBy(col(\"timestamp\").desc())\n",
    "\n",
    "df_top10 = (\n",
    "    df_recent.groupBy(\"timestamp\", \"job_field\")\n",
    "    .agg({\"quantity\": \"sum\"})\n",
    "    .withColumnRenamed(\"sum(quantity)\", \"job_count\")\n",
    "    .sort(col(\"timestamp\").desc())\n",
    "    .withColumn(\n",
    "        \"row_num\",\n",
    "        row_number().over(\n",
    "            Window.partitionBy(\"timestamp\").orderBy(col(\"job_count\").desc())\n",
    "        ),\n",
    "    )\n",
    "    .filter(col(\"row_num\") <= 10)\n",
    "    .drop(\"row_num\")\n",
    "    .sort(col(\"timestamp\").desc(), col(\"job_count\").desc())\n",
    ")\n",
    "\n",
    "display(df_top10)\n",
    "\n",
    "df_top10.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\n",
    "    \"dev.job_prospects.fct_top_10_recent_job\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea35b1cc-3bab-4861-b103-dda4ca497c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aggregate_job_prospects",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
